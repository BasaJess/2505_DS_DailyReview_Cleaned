# Day 18, 19.02.2025

##  __Basic Overview__

*  [Feature Engineering tools](#feature-engineering-tools)
* [Feature Engineering in the ML process](#feature-engineering-in-the-ml-process)
* [Advanced Feature Engineering](#advanced-feature-engineering)


---
##  __Schedule__

|Time|Content|
|---|---|
|09:50 - 10:00|Review|
|10:00 - 12:00|Explanation of TopUp Feature Engineering (Notebooks 1 + 2)|
|12:00 - 12:30|Working on Notebooks|
|12:30 - 13:30|Lunch Break| 
|13:30 - 16:00|Working on Notebooks|
|16:00 - 17:00|Standup|
|17:00 - open-end|Working on Notebooks|

---

# Feature Engineering Techniques

Feature engineering is crucial for preparing data for machine learning or analysis. Here's an overview of key techniques:

## 1. Imputation

Used to replace NaN (Not a Number) values.

### a. SimpleImputer
- Replaces NaN with a specified value based on a chosen strategy.
- Strategies include: mean, median, most_frequent, constant.

### b. KNNImputer
- Uses K-Nearest Neighbors to impute missing values.
- Considers a specified number of neighbors to calculate the replacement value.

## 2. Categorical Encoding

Converts **categorical** variables into **numerical** format.

### a. get_dummies (pandas)
- Creates binary columns for each category.
- Also known as one-hot encoding.

### b. OneHotEncoder (scikit-learn)
- Similar to get_dummies, but returns a sparse matrix.
- Works well with variables that have many unique values.

## 3. Feature Scaling

Adjusts features to a common scale, improving model performance and comparability. Useful for features that have very different scales, such as age (0-100) and income (0-1,000,000). Does not change the shape of distribution.

### a. Standardization
- Transforms features to have a mean of 0 and a standard deviation of 1 (Center values around 0).
- Implemented in scikit-learn as StandardScaler().

### b. Normalization
- Scales values to a fixed range, between 0 and 1.
- Sensitive to outliers.
- Preserves zero values.
- Implemented in scikit-learn as MinMaxScaler().

## 4. Feature Expansion

Creates new features from existing ones to capture complex relationships.

### a. Polynomial Features
- Generates polynomial and interaction features.
- Useful for modeling non-linear relationships.

## 5. Discretization

Converts continuous variables into discrete categories or bins. Implemented in scikit-learn as KBinsDiscretizer().


# Feature Engineering in the ML process

#### 1. Perform initial Exploratory Data Analysis (EDA)
- Understanding the data better

#### 2. Split dataset into train and test sub-samples as early as possible!

- Rule of Thumb: 80% used as Training-Data and 20% as Test-Data
- Test-Data: How good can our model predict the target?

![alt text](image.png)

#### 3. Separate features (X) and target variable (y)

- It is also possible to do this before step 2.

#### 4. Define numerical and categorical features

#### 5. Feature Engineering

- Transform the train data using different processing techniques
- For training data:
    - Use fit() to learn parameters from the training data
    - Use transform() to apply the transformation
    - Possible to use fit_transform() in one step

#### 6. Apply feature engineering transformations to both train and test data!

- Apply same transformations like for the training data to the test data
- Ensures consitency between training and test sets
- For test data:
    - Important to only use transform() method with parameters learned from the training data

#### 7. Build the model
- instantiate the model 
- train the model with fit()

#### 8. Validate the model
- Calculation of R2 with score() using training data

#### 9. Evaluate the model
- Calculation of R2 with score() using test data

# Advanced Feature Engineering

Help organize and streamline the data preparation process, making it easier to consistently apply transformations to both training and test data.

1. **Pipeline**
   - Combines machine learning steps in a series
   - Executes preprocessing steps in a specific sequence
   - Used for training and testing data

   ![alt text](image-2.png)

2. **ColumnTransformer**
   - Combines machine learning steps in parallel
   - Applies different tools to modify specific columns (e.g. numerical and categorical)
   - Returns processed data in column format

   ![alt text](image-1.png)

   https://medium.com/@pujalabhanuprakash/understanding-the-difference-between-column-transformation-and-pipeline-in-scikit-learn-4b7fb252b52e
