
# Week 5: 24.02.2025 - Evaluation Metrics
<span style="color:white">

![Confusion](https://miro.medium.com/v2/resize:fit:800/format:webp/0*zzF9lZtzX5p72IBZ.jpeg)

1) Theoretical input about evaluation metrics <br>

2) Practice with the repository [ds-evaluation-metrics](https://github.com/neuefische/ds-evaluation-metrics "ds-evaluation-metrics") 
</span>

---
## <span style="color:white"> __Basic Overview__ </span>
 
* <span style="color:white"> Confusion Matrix
* <span style="color:white"> Accuracy
* <span style="color:white"> Precision and Recall
* <span style="color:gwhite"> F1-score
* <span style="color:white"> ROC-Curve


---
## <span style="color:white"> __Schedule__ </span>
<span style="color:gwhite">

|Time|Content|
|---|---|
|09:00 - 10:00| Daily review |
|10:00 - 11:15| Teaching: Evaluation Metrics|
|11:15 - 12:00| Evaluation metrics notebooks|
|12:00 - 13:00| Lunch break|
|13:00 - 16:00| Evaluation metrics notebooks|
|16:00 - 16:30| Daily standup|

---
## <span style="color:white"> __Regression vs. Classification__ </span>
<span style="color:white">

![Classification](https://neuefische.github.io/nf-ds-onl-en-2701-book/images/evaluation_metrics/img_p5_2.png) 

- **Regression**: Predicting continuous numerical values
- **Classification**: Predicting categories

## <span style="color:white"> __Confusion Matrix__ </span>
<span style="color:white">

![Confusion_matrix_meme](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C-l2-koSFr1mpdiww3MogQ.png) 

- Matrix used to evaluate the performance of a classification model
- Compares actual labels vs. predicted labels
- Positive: The label/ class/ category of interest



</span>

---
## <span style="color:white"> CM Metrics </span>
<span style="color:white">

![Metrics](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d0UCCIF10Soi7VQGxdVrWQ.jpeg)

1) **Accuracy**: <br> How many did you get right?
- Issue with imbalanced dataset: When one case is very rare, it leads to false conclusions
- Example: We have 1 dog and 99 cats. If the model predicts all animals as cats, the accuracy is 99%

<br>

2) **Sensitivity (Recall)**: <br>
 How many of the positive values are correctly predicted by your model?
- Determines the **True Positive Rate** of the classifier
- Can be cheated by guessing everything is positive

<br>

3) **Precision**: <br> 
How many of the predicted positives were truly positive? 
- Negative values are left out of the picture


## <span style="color:white"> Tweaking the model</span>
<span style="color:white">

- We can set a threshold: The lower the threshold, the more instances get predicted positive

![Metrics](https://www.landtiere.de/assets/images/30/211/30211751-katze-hund-teppich-2oIqPVjmF7ec.jpg)


Example: 10 dogs, 90 cats.
- If threshold is **effectively zero**: The model predicts every animal as dog 
- Recall will be 1 (all dogs will be identified correctly)
- The precision is equal to the share of the positives, i.e.,  only 10% because 90 out of 100 dog predictions were actually cats

![Recall - Precision](https://neuefische.github.io/nf-ds-onl-en-2701-book/images/evaluation_metrics/precision_recall.png)

High recall or high precision? <br> Depends on the business question

## <span style="color:white"> F1-Score</span>
<span style="color:white">

**Harmonic mean of precision and recall**

$F_1 = 2\cdot\frac{Precision\;\cdot\;Recall}{Precision\;+\;Recall}$
- If precision and recall are somewhat high, the F1-score is close to 1 indicating a good model
- If one of them is low, the F1-score will decrease  


## <span style="color:white"> Receiver Operating Characteristic Curve (ROC Curve)</span>
<span style="color:white">

![Metrics](https://databasecamp.de/wp-content/uploads/ROC-Curve-Diagram.png)

- ROC curve is a plot of performance across all possible thresholds
- Random classifier: ROC AUC is 0.5

- Perfect classifier:
ROC AUC is 1

- Influences: Class imbalance, over- and underfitting 


## <span style="color:white"> Modules</span>
<span style="color:white">


`from sklearn.metrics`:

    import confusion_matrix
    import accuracy_score
    import recall_score
    import precision_score
    import f1_score
    import roc_curve
    import roc_auc_score
    import classification_report

What if we have more than two categories? 

![Hund, Katze und Hase](https://www.br.de/radio/bayern2/hund-hase-katze-100~_v-img__16__9__xl_-d31c35f8186ebeb80b0cd843a7c267a0e0c81647.jpg?version=dcaf7)


    y_true = ["cat", "dog", "cat", "cat", "dog", "rabbit"]
    y_pred = ["dog", "dog", "cat", "cat", "rabbit", "cat"
    confusion_matrix(y_true, y_pred, labels=["dog", "cat", "rabbit"])

    array([[1, 0, 1],
       [1, 2, 0],
       [0, 1, 0]])  


|Actual vs predicted| Predicted dog| Predicted cat | Predicted rabbit |
|---|---|---|---|
|Actual dog| 1 | 0 | 1|
|Actual cat| 1| 2 | 0 |
|Actual rabbit| 0| 1 | 0|

<br>

    print(classification_report(y_true, y_pred))


                  precision    recall  f1-score   support

         cat       0.67      0.67      0.67         3
         dog       0.50      0.50      0.50         2
      rabbit       0.00      0.00      0.00         1

    accuracy                           0.50         6
    macro avg       0.39     0.39      0.39         6
    weighted avg    0.50     0.50      0.50         6
