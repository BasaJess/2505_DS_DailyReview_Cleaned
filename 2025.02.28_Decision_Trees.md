# Friday week 6, 28.02.2025
<span style="color:grey">

---
## <span style="color:black"> __Basic Overview__ </span>
 

* <span style="color:black">  Decision Tree
* <span style="color:black"> Gini Impurity
* <span style="color:black"> Regression Tree
---
##  __Schedule__
<span style="color:black">

|Time|Content|
|---|---|
|09:00 - 10:00|Daily review|
|10:00 - 12:00|Desicion tree lecture |
|12:00 - 13:00|Lunch break|
|13:00 - 16:00|Decision tree Exercise notebooks| 
|16:00 - 16:30|Stand up|
|16:30 - 18:00|Exercises and Ending||

---
## <span style="color:black"> __What is a Decision Tree?__ </span>
<span style="color:black">

A Decision tree is a data structure consisting of a hierarchy of nodes that can be used for supervised learning and unsupervised learning problems (classification, regression, clustering, …).

Decision trees use various algorithms to split a dataset into homogeneous (or pure) sub-nodes.

## <span style="color:black"> __Advantages of decision trees__ </span>

Decision tree algorithms have multiple advantages:

* Simple to understand and interpret
* Flexible as they can describe non-linear data
* Simple to use as no data preprocessing needed.

On the other hand, trees do have some disadvantages:

* Sensitive to small variations in the training data
* Susceptible to overfitting when unconstrained

</span>

---
## <span style="color:black"> Understanding the Decision Tree Algorithm

Decision trees are simple models that have branches, nodes and leaves and break down a dataset into smaller subsets containing instances with similar values.


### Decision Trees Definitions


 Root node:  First node in the path from which all decisions initially started from. It has no parent node and 2 children nodes

Decision nodes: Nodes that have 1 parent node and split into children nodes (decision or leaf nodes)

Leaf nodes: Nodes that have 1 parent, but do not split further (also known as terminal nodes). They are the nodes that produce the prediction.

Branches: A subsection of the entire tree (also known as a sub-tree)

Parent / child nodes: A node that is divided in sub-nodes is called a parent node. The sub-nodes are the child nodes of the parent from which they divided.

Maximum depth: maximum number of branches between the top and the lower end

![](https://www.jcchouinard.com/wp-content/uploads/2021/11/image-1-1024x929.png)
</span>

<span style="color:black">

</span>
---

## <span style="color:black"> __Decision Tree Metrics__ </span>

Decision trees try to produce the purest leaves in a recursive way by splitting nodes into smaller sub-nodes.

But how does it choose how to split the nodes, evaluate the purity of a leaf, or decide when to stop?





---
## <span style="color:black"> __Gini Impurity__ </span>

<span style="color:black">

Gini impurity is used  to compute the homogeneity of a leaf in a less computationally intensive way.

The purer, or homogenous, a node is, the smaller the Gini impurity is
Gini is the sum of squares of probabilities for each class.
![](https://www.jcchouinard.com/wp-content/uploads/2021/11/image-22-768x614.png)

And, then, again, the model will estimate the purity of the split by
 computing the weighted Gini impurity of both
 children leaves compared to the Gini impurity of the parent.
![](https://www.jcchouinard.com/wp-content/uploads/2021/11/image-23-768x883.png)
</span> 

---

## <span style="color:black"> __Regression Trees__ </span> 
* #### Regression trees are similar to decision trees but have leaf nodes which represent real values.

Suppose we 
are scientists and have developed a brand new drug to treat the common flu.
![](https://miro.medium.com/v2/resize:fit:1400/1*bjyQC6A8dhYuKcFC_iunYw.png)



However, we don’t know the optimal dosage for our patients. To investigate this problem we run a clinical trial with different dosages and measure how effective each dosage is. In the end we would like to accurately predict the efficiency of the drug at a certain dosage level.
Linear Regression
If we plot the results of the clinical trial in some hypothetical scenario, the data points may look similar to the graph below.

![](https://miro.medium.com/v2/resize:fit:1400/1*lr4rzUCDO7cb3wWKvrSnUg.png)

The data points in the plot above (Plot A) indicates that there is some positive correlation between drug dosage and drug
 efficiency. Meaning that in general, the higher the dose, the higher the efficiency. We could easily fit a straight line 
 to this data with linear regression and use the line of best fit to draw predictions. For instance, a drug dosage of 23 mg 
 has a predicted value of 63% efficiency. Unfortunately, data doesn’t always seem to present itself so well. More realistically, we might end up with data points being much noisier. This is seen in the plot below.

![](https://miro.medium.com/v2/resize:fit:1400/1*HqqTwl1dB2XrFLyHU_91IQ.png)


Applying linear regression to the data points (Plot B) above, we notice that there is a large difference between the predicted value and the actual value for a drug dosage of 23 mg. It seems evident that linear regression might not be the best method to model the data. Well, what other method could we use? Yep, you guessed it, it’s in the title: regression trees.


#### Building our own Regression Tree
Now that we have gone through an example of what a regression tree looks like, let us develop one ourselves from the very beginning using the same unstructured data in Plot B. The first part of building a regression tree is deciding which threshold to have in the root node.

![](https://miro.medium.com/v2/resize:fit:1400/1*0gJLnRuyKtryaB01CaQqHA.png)

To help us decide, we will first focus on the observations with the two smallest dosages. The average dosage between those two patients is 3 mg. We draw a vertical line at the point 3 to indicate a split in our data.
![](https://miro.medium.com/v2/resize:fit:1400/1*NPCcV6xp6gN-l-EY6Re35g.png)

The two dots which are highlighted red represent the two smallest dosages. The red dotted line splits the data into two parts. The next step is to calculate the average efficiency of the observations on the left and right hand side of the red dotted line. On the left hand side (less than 3 mg), there is only one observation, which results in an average of 0%. On the right hand side (greater than 3mg), there are many observations with an average of 38.8.

![](https://miro.medium.com/v2/resize:fit:1400/1*Wj36hIG-wFWuHyK17pHC7Q.png)

We create a simple tree with “Dosage < 3” as the root node and two subsequent leaf nodes. The average on the left hand side of the dotted line goes into the left leaf node and the average on the right hand side goes to the right leaf node. The values in the leaf nodes are the predictions that this simple tree will make for drug effectiveness.

How to determine how well our simple tree splits the data ?

The left leaf node has predicted the outcome perfectly. The actual efficiency for dosages less than 3mg is 0% and our tree predicted it as such. But how do we check the accuracy of the right hand side when there are so many observations to consider?

We can use a method that is common in linear regression:

Sum of Squared Residuals (SSR)

A residual is a measure of the distance from a data point to a regression line. SSR measures the overall difference between our data and the values predicted by our regression tree. Generally, a lower SSR indicates that the regression model can better explain the data while a higher SSR indicates that the model poorly explains the data. The formula of SSR:

![](https://miro.medium.com/v2/resize:fit:237/1*TEcgOkHbFbiSoHgQ_MTjkQ.png)

We can visualise the residuals of our simple tree by constructing lines from the observed to the predicted value. Notice again that on the left hand side the predicted value is equal to the observed value.
![](https://miro.medium.com/v2/resize:fit:1400/1*4yP6HKADZhWV4yYh1eVX5Q.png)
We can use the residuals to quantify the quality of the predictions made by our simple tree. Next we calculate the SSR for the tree by adding the SSR of the left and right leaf nodes.
![](https://miro.medium.com/v2/resize:fit:1360/1*_Ko1JgHk1Q_DUPOsPjeZDw.png)

We get a total SSR value of 27.5 for the tree. The entire process is then applied to the second and third lowest observations, the third and fourth lowest observations and so on..

Once we have calculated all the SSR values for the trees made by the pairs of observations, we can plot the SSR values as a function of dosage threshold.
![](https://miro.medium.com/v2/resize:fit:1326/1*cYMZIUI5JpsfhCSsYCFIUQ.png)

The lowest SSR value represents the dosage threshold which will be at the root node of the tree. In this case the lowest SSR value is 14.

Once the root node has been decide, the data will be split up into a left and right node. These nodes also require an optimal threshold to split the data further. How do we go about choosing these thresholds? The same process which was applied to obtain the root node is now applied to the remaining nodes of the tree. Each node should have a threshold which represents the lowest SSR value available. Once the data cannot be split further (only one observation in the train data) or there it is redundant to split (if all observations have the same value), the node itself becomes the leaf node.

## <span style="color:black"> __PRUNING A TREE IMPROVES GENERALIZATION__


one way to prevent overfitting a Regression Tree to the Training Data is to remove some of the leaves 
and replace the split with a leaf that is the average of the larger number of observations, 
so that a tree do a better job on testing data.

Weekest Link pruning works by calculating the tree score that is based on sum of the square residuals 
for the tree or sub tree and a Tree complexity penalty that is the function of number of leaves or Terminal nodes 
The tree complexity penalty compensates for the differnece in the number of leaves ,a(alpha) is a tunning parameter 
that can be find using the cross validation .
It is obtained by calculating  the sum of squared residuals with trees with fewer leaves.

![](https://developers.google.com/static/machine-learning/decision-forests/images/ShouldWePrune.png)

![](https://isip.piconepress.com/courses/msstate/ece_8463/lectures/current/lecture_27/lecture_27_07_01.jpg)
