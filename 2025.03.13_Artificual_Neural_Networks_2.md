# Artificial Neural Networks 2

#### Review of Thursday, Week 7, 13.03.2025, Annette

<img src="https://res.cloudinary.com/dry8rzbyx/image/fetch/s---RDuZ9Xt--/f_auto/q_auto/c_scale,w_1536/https://www.knime.com/sites/default/files/public/1-intro-deep-neural-networks.png" alt="Neural Network" width="40%">

---
## <span style="color:Black"> __Summary__ </span>
 

* <span style="color:grey"> Playing with ANN configurations
* <span style="color:grey"> ANN vocabulary and functionalities
* <span style="color:grey"> Coding ANN with Keras

---
##  __Schedule__
<span style="color:grey">


|Time|Content|
|---|---|
|09:00 - 09:30|Daily Review|
|10:00 - 10:30|[Neural Network Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.33333&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)|
|10:30 - 11:30|Assignment: ANN vocabulary |
|              |Lunch Break| 
|13:45 - 16:00|03_Overfit_Underfit.ipynb, <br> 04_Load_saved_Models.ipynb  |
|16:00 - 16:40|Stand-up|
|13:45 - 16:00|03_Overfit_Underfit.ipynb,<br> 04_Load_saved_Models.ipynb  |

![alt text](https://www.fairplanet.org/wp-content/uploads/2021/12/credit-loreto-oyarte-galvez-caption-high-resolution-mycelium-network-1-534x391.1639150064.jpg)
---
## <span style="color:grey"> __[Neural Network Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.33333&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)__ </span>
<span style="color:grey">

![morning](images/playground.png) 

## <span style="color:black"> __Assignment: Items and functionalities of ANN:__ </span> 
1. ***Architecture***: 
    - Layers and neurons: types and number
    - Epoch (and batch)
    - Loss functions, e.g. mse, log-loss, cross entropy loss
2. ***Activation functions***: When do we use them and where in the network? advantages + disadvantages of 
    - Linear
    - Relu + family
    - TanH
    - Sigmoid
    - Softmax
3. ***Optimizers***: What are the differences? How do we choose one?
    - Gradient descent
    - Adam
    - Momentum
    - RMSPRop) 
4. ***Regularization***: How can we stop our network from overfitting?
    - Drop out
    - Early stopping
    - Batch normalisation, 
    - How does L1/L2 work in ANN?  

---

## <span style="color:black"> __1. Architecture__ </span>



## Neurons and layers

You can even combine deep and wide networks!
<img src="https://www.researchgate.net/profile/Kaveh-Bastani/publication/328161216/figure/fig3/AS:679665219928064@1539056224036/llustration-of-the-wide-and-deep-model-which-is-an-integration-of-wide-component-and.ppm" alt="Neural Network" width="99%">

(Bastani, Kaveh & Asgari, Elham & Namavari, Hamed. (2018). Wide and Deep Learning for Peer-to-Peer Lending. 10.48550/arXiv.1810.03466. )

##  Epoch

<p align="center">
    <img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6YV20LCdYgkCXgkz19hKzApXa6NXbF3kZazwuCb8Ic0OROtF3LIClHrVQduufDS6dhZ5OB-dEV4IW4D6rz-Cui-1OehB0VFQIBlD8Beb0x_z15IzlLLDLiBiRMf163JmtbUMXZWaxjLKT/s1600/tumblr_lzbiewJvNT1qfsqzho1_1280.jpg" alt="Image 1" width="30%">
    <img src="https://media.geeksforgeeks.org/wp-content/uploads/20241024155237307614/epoch-in-machine-learning_.webp" alt="Image 2" width="69%">
</p>
--

## Loss functions
<img src="https://pbs.twimg.com/media/Dgzo42jUwAAzb61?format=jpg&name=4096x4096" alt="Neural Network" width="99%">

[Source](https://x.com/DynamicWebPaige/status/1012440443823349760/photo/1)


## <span style="color:black"> __2. Activation functions__ </span>

***Most commonly used in hidden Layers***:

1. Rectified Linear Activation (***ReLU***)
2. Logistic (***Sigmoid***)
3. Hyperbolic Tangent (***Tanh***)

<img src="https://statusneo.com/wp-content/uploads/2023/08/Credit_KAGGLE_1.jpg" alt="Neural Network" width="60%">


<img src="https://lh4.googleusercontent.com/hTeaMXYrsBlpKrGvRCvSX8maYuU4Zhd9-6B_Z3QjnnpE02MhfFK8IHgrDsX9U9SoSw9MIJFQbQyR64PHqNjGfMa8LgUctX5ht0Z21NxqJ-AAd5bU30mFGaTzNhiNuiwO2OVvpfYYFAonf3k8wQTqwGA">

<img src="https://media.licdn.com/dms/image/v2/C4E12AQEwXwy5TWLE0Q/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1620787453948?e=1747267200&v=beta&t=oRf0wSmg3xgprPw6ZXjsZGh35p98cO52UY1vn_alCE4">

[Source](https://www.linkedin.com/pulse/activation-functions-neural-networks-leonardo-calderon-j-/)




***Output Layers activation function depends on problem***:

<img src="https://machinemindscape.com/wp-content/uploads/2024/02/Best-Activation-Functions-for-Output-Layers-1-1024x536.png">

[Source](https://machinemindscape.com/how-to-choose-the-best-activation-functions-for-hidden-layers-and-output-layers-in-deep-learning/)

---
## <span style="color:black"> __3. Optimizers__ </span>
Optimizers are algorithms that ***adjust a model's weights to minimize the loss***  function during training. They guide how weights are updated in response to the calculated gradients from backpropagation. 
![optimizers_overview](https://miro.medium.com/v2/resize:fit:640/format:webp/1*SjtKOauOXFVjWRR7iCtHiA.gif)


## Classic Optimization Algorithms

- **1847 - Gradient Descent**: Introduced by Augustin-Louis Cauchy.  
- **1986 - Momentum (SGD)**: Developed by David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams.  
- **2012 - RMSProp**: Proposed by Geoffrey Hinton in his lecture notes.  
- **2014 - Adam (Adaptive Moment Estimation)**: Introduced by Diederik P. Kingma and Jimmy Ba.  

## Nature inspired Optimizer Algorithms
<p align="center">
    <img src="https://www.researchgate.net/profile/Tansel-Doekeroglu/publication/335676329/figure/fig5/AS:800364118945794@1567833084463/The-bubble-net-feeding-behavior-of-a-humpback-whale.ppm" alt="Image 1" width="40%">
    <img src="https://imageio.forbes.com/specials-images/imageserve/66db1066af07992c39acd388/Solitary-bubble-netters/0x0.jpg?format=jpg&crop=1154,865,x0,y143,safe&width=1440" alt="Image 2" width="59%">
</p>

***2008*** ***Firefly Algorithm (FA)***:  
   - Fireflies communicating using light intensity  
   - **Original Purpose**: Solve complex optimization problems.  
   - **Now Used For**: Image processing, feature selection, and clustering.  

***2009*** ***Cuckoo Search (CS)***:  
   - Cuckoo birds laying eggs in other birds' nests  
   - **Original Purpose**: Find optimal solutions using a mix of random exploration and targeted search.  
   - **Now Used For**: Engineering design, scheduling, and machine learning hyperparameter tuning.    

***2016*** ***Whale Optimization Algorithm (WOA)***:  
   - Bubble-net hunting strategy of humpback whales  
   - **Original Purpose**: Solve continuous optimization problems.  
   - **Now Used For**: Deep learning optimization, feature selection, and medical image processing.  

---

## <span style="color:black"> __4. Regularization__ </span> 
We can add a ***penalty term to the loss function*** during training.

We can ***add noise to the inputs, outputs, or gradients of the network*** to prevent a neural network from overfitting on the training data
![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*ZqWv_y7royVefhNsU5nJZA.jpeg)

## Dropout
During each training iteration, ***dropout randomly deactivates a subset of neurons*** in a layer. The probability of a neuron being dropped is determined by a ***hyperparameter called the dropout rate***

For example, a dropout rate of 0.5 means that each neuron has a 50% chance of being deactivated. The remaining active neurons are scaled up by a factor of $(1 - \text{dropout rate})^1$ to maintain the overall output magnitude

![optimizers_overview](https://media.geeksforgeeks.org/wp-content/uploads/20240702144440/Neural-Networks-with-Dropout-for-Effective-Regularization.webp)
[Source](https://www.geeksforgeeks.org/training-neural-networks-with-dropout-for-effective-regularization/)


## Early stopping
Stopping the training when the training error is no longer decreasing but ***the validation error is starting to rise***.
![optimizers_overview](https://theaisummer.com/static/7a6353ed78b045f32e4ac39b0b4d66d2/a878e/early-stopping.png)

## Batch Normalization

- If we are able to somehow ***normalize the activations from each previous layer*** then the gradient descent will converge better during training. 
- It is another network layer that gets inserted between a hidden layer and the next hidden layer
- Like the parameters (eg. weights, bias) of any network layer, a Batch Norm layer also has parameters of its own:
    - Two learnable parameters called ***beta and gamma***.
    - Two non-learnable parameters (Mean Moving Average and Variance Moving Average) are saved as part of the ‘state’ of the Batch Norm layer.

Normalized values for each activation feature vector using the corresponding mean and variance: mean=0 and variance=1. 

![optimizers_overview](https://towardsdatascience.com/wp-content/uploads/2021/05/04uvh_GjZWzmSX4AX-1024x158.png)

- **Faster Convergence & Training**: Batch Norm speeds up training and makes models less sensitive to weight initialization and hyperparameter tuning.
- **Higher Learning Rates**: Allows the use of higher learning rates by reducing the effect of outlier gradients, which would otherwise hinder gradient descent.
- **Reduced Gradient Sensitivity**: Dampens the effect of random weight initialization, preventing slow convergence due to outlier gradients.
- **When Not Applicable**:
  - **Small Batch Sizes**: Batch Norm struggles with high noise in mean and variance from small batches.
  - **Recurrent Networks**: In recurrent networks, varying activations make Batch Norm impractical.


[Source](https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739/)

([Ioffe & Szegedy, 2015, "Batch normalization: Accelerating deep network training by reducing internal covariate shift." International conference on machine learning. pmlr.](https://arxiv.org/pdf/1502.03167))


![yay](https://www.travelandleisure.com/thmb/ipaUM2aRMRb87KF01m4x4ytwxXY=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/hallerbos-forest-halle-belgium-2-BEAUTFORESTS0721-4ff5b556613e4814b5b7165f8851de39.jpg)