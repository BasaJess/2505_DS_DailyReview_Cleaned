# day , 18.03.2025
jonas  

---

## __basic overview__  

* Dimensionality Reduction and Clustering in Data Science  

---

##  _schedule__  

|Time|Content|  
|---|---|  
|09:00 - 10:00|daily review|  
|10:00 - 11:30|exploring dimensionality reduction|  
|11:30 - 12:30|applying PCA in Python|  
|12:30 - 13:30|lunch break|  
|13:30 - 15:00|exploring clustering techniques|  
|15:00 - 17:00|k-means and hierarchical clustering|  

---

## Dimensionality Reduction  

Dimensionality reduction is a technique used to simplify data by reducing the number of variables while retaining as much information as possible.  

### **Principal Component Analysis (PCA)**  
- PCA is one of the most popular techniques for dimensionality reduction.  
- It transforms correlated features into a set of linearly uncorrelated features called principal components.  
- These components capture the maximum variance in the data.  

#### **Steps in PCA:**  
1. Standardize the data (mean = 0, variance = 1).  
2. Compute the covariance matrix.  
3. Compute the eigenvalues and eigenvectors.  
4. Select the top k eigenvectors (principal components).  
5. Transform the original dataset using these components.  

#### **Implementation in Python:**  
```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np

# Sample dataset
X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])

# Standardizing the dataset
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Applying PCA
pca = PCA(n_components=1)
X_pca = pca.fit_transform(X_scaled)
print(X_pca)
```

---

## Clustering  

Clustering is an unsupervised machine learning technique used to group similar data points together.  

### **Types of Clustering:**  
- **K-Means Clustering**: Divides data into k clusters by minimizing the variance within each cluster.  
- **Hierarchical Clustering**: Builds a hierarchy of clusters using either an agglomerative (bottom-up) or divisive (top-down) approach.  
- **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: Groups points based on density, making it useful for detecting noise/outliers.  

---

### **K-Means Algorithm:**  
1. Choose the number of clusters (k).  
2. Randomly initialize k cluster centroids.  
3. Assign each data point to the nearest centroid.  
4. Recalculate centroids based on assigned points.  
5. Repeat steps 3-4 until centroids stabilize.  

#### **Implementation in Python:**  
```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np

# Sample dataset
X = np.array([[1, 2], [2, 3], [3, 4], [8, 7], [9, 6], [10, 8]])

# Applying K-Means
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(X)
labels = kmeans.labels_
centroids = kmeans.cluster_centers_

# Plot results
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200)
plt.show()
```

---

### **Hierarchical Clustering:**  
- Builds a hierarchy of clusters.  
- **Agglomerative clustering**: Merges the closest clusters iteratively.  
- **Divisive clustering**: Starts with all points in one cluster and splits iteratively.  

#### **Example in Python:**  
```python
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Sample dataset
X = np.array([[1, 2], [2, 3], [3, 4], [8, 7], [9, 6], [10, 8]])

# Creating linkage matrix
linked = linkage(X, 'ward')

# Plot dendrogram
plt.figure(figsize=(8, 5))
dendrogram(linked)
plt.show()
```

---

### **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**  

DBSCAN is a clustering method that groups points based on density.  
Unlike K-Means, **it does not require specifying the number of clusters** and is useful for detecting **outliers**.  

#### **Main Parameters:**  
- **Îµ (epsilon)**: Defines the maximum radius around a point to consider its neighbors.  
- **min_samples**: Minimum number of points required to form a dense region.  
- Points classified as **-1** in the output are **outliers (noise points)**.  

#### **Implementation in Python:**  
```python
from sklearn.cluster import DBSCAN
import numpy as np
import matplotlib.pyplot as plt

# Sample dataset
X = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]])

# Applying DBSCAN
dbscan = DBSCAN(eps=1.5, min_samples=2)
labels = dbscan.fit_predict(X)

# Plot results
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolors='k')
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title("DBSCAN Clustering")
plt.show()

# Print cluster labels
print("Cluster Labels:", labels)
